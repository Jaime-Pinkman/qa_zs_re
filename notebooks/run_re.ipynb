{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = datasets.load_from_disk(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vblagoje/bart_lfqa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Is the {} involved in the development or progression of {}?\",\n",
    "    \"Does the {} have a known association with the {}?\",\n",
    "    \"Are there any studies that suggest a connection between the {} and the {}?\"\n",
    "]\n",
    "positive_candidates = [\n",
    "    \"{} is strongly implicated in the development or progression of {}\",\n",
    "    \"{} has a moderate association with the {}\",\n",
    "]\n",
    "negative_candidates = [\n",
    "    \"The relationship between {} and {} is uncertain or unclear\",\n",
    "    \"{} has no known connection to the {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(model, tokenizer, question, context):\n",
    "    conditioned_doc = \"<P> \" + \" <P> \".join([d for d in [context]])\n",
    "    query_and_docs = \"question: {} context: {}\".format(question, conditioned_doc)\n",
    "\n",
    "    model_input = tokenizer(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    generated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),\n",
    "                                           attention_mask=model_input[\"attention_mask\"].to(device),\n",
    "                                           min_length=64,\n",
    "                                           max_length=256,\n",
    "                                           do_sample=False, \n",
    "                                           early_stopping=True,\n",
    "                                           num_beams=8,\n",
    "                                           temperature=1.0,\n",
    "                                           top_k=None,\n",
    "                                           top_p=None,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           no_repeat_ngram_size=3,\n",
    "                                           num_return_sequences=1)\n",
    "    return tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_candidates(positive_candidates, negative_candidates, zh_res):\n",
    "    positive_prob, negative_prob = 0, 0\n",
    "    for label, score in zip(zh_res[\"labels\"], zh_res[\"scores\"]):\n",
    "        if label in positive_candidates:\n",
    "            positive_prob += score\n",
    "        elif label in negative_candidates:\n",
    "            negative_prob += score\n",
    "    return [\n",
    "        positive_prob, \n",
    "        negative_prob,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_answer(dataset, i, pipe, clf, questions, positive_candidates, negative_candidates):\n",
    "    # store preds for rels\n",
    "    y_preds, ys = [], []\n",
    "\n",
    "    # create candidate_labels\n",
    "    candidate_labels = positive_candidates + negative_candidates\n",
    "\n",
    "    # store norm to name - type mapping\n",
    "    norm2text = {}\n",
    "    for norm, span, type in zip(dataset['test'][i]['ner_norms'], dataset['test'][i]['spans'], dataset['test'][i]['ner_labels']):\n",
    "        norm2text[norm] = f\"{dataset['test'][i]['text'][span[0]:span[1]]} {type}\"\n",
    "    \n",
    "    # iterate over relation pairs with label\n",
    "    for rel, label in zip(dataset['test'][i]['relations'], dataset['test'][i]['relations_labels']):\n",
    "        # create candidates for zs\n",
    "        candidates = []\n",
    "        for c in range(len(candidate_labels)):\n",
    "            candidates.append(candidate_labels[c].format(norm2text[rel[0]], norm2text[rel[1]]))\n",
    "        # make predictions per question\n",
    "        probs_per_question = []\n",
    "        # iterate over questions\n",
    "        for question in questions:\n",
    "            # create QA input and get QA model's output\n",
    "            qa_res = get_answer(\n",
    "                pipe, \n",
    "                tokenizer, \n",
    "                question.format(norm2text[rel[0]], norm2text[rel[1]]),\n",
    "                dataset['test'][i]['text']\n",
    "            )\n",
    "\n",
    "            # get classification results\n",
    "            zh_res = clf(qa_res[0], candidates, multi_label=False)\n",
    "            # combine probabilities\n",
    "            y_pred = process_candidates(\n",
    "                [c.format(norm2text[rel[0]], norm2text[rel[1]]) for c in positive_candidates], \n",
    "                [c.format(norm2text[rel[0]], norm2text[rel[1]]) for c in negative_candidates], \n",
    "                zh_res)\n",
    "            probs_per_question.append(y_pred)\n",
    "        y_preds.append(probs_per_question)\n",
    "        ys.append(int(label))\n",
    "    return y_preds, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(dataset, num, pipe, clf, questions, positive_candidates, negative_candidates):\n",
    "    y_preds, ys = [], []\n",
    "    for i in tqdm(range(num)):\n",
    "        y_pred, y = classify_answer(dataset, i, pipe, clf, questions, positive_candidates, negative_candidates)\n",
    "        y_preds.extend(y_pred)\n",
    "        ys.extend(y)\n",
    "    return y_preds, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, ys = predict_data(\n",
    "    input_dataset, len(input_dataset), \n",
    "    model, classifier, \n",
    "    questions, \n",
    "    positive_candidates, negative_candidates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ys, [1 if (y[0][0]+y[1][0]) > 1 else 0 for y in y_preds]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
